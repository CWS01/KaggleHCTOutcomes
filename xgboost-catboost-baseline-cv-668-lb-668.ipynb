{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":70942,"databundleVersionId":10381525,"sourceType":"competition"},{"sourceId":211253469,"sourceType":"kernelVersion"},{"sourceId":211322530,"sourceType":"kernelVersion"}],"dockerImageVersionId":30805,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# XGBoost CatBoost Baseline - CV 0.688 - LB 0.688\nIn this notebook, we present an XGBoost and CatBoost baseline. These GBDT are very fast to train on GPU! We achieve CV 0.666 and CV 0.665 for each model respectively and the ensemble achieves CV 0.668. There is a discussion about this notebook [here][2]\n\nWe tranform the two train targets into a single target and then train regression models. We load Kaggle's official metric code from [here][1] and evaluate the CV performance of each model. Neither model in this notebook uses any feature engineering, so we can boost the CV and LB score by adding feature engineering.\n\nAdditionally to boost CV and LB, we can try building NN models (like MLP) and ML models (like RAPIDS SVC/SVR). Furthermore, there are many different ways to encode the two train targets and train models. We can study the competition metric and determine the best way to train models that optimize this competition's metric.\n\nGood luck, have fun!\n\n[1]: https://www.kaggle.com/code/metric/eefs-concordance-index\n[2]: https://www.kaggle.com/competitions/equity-post-HCT-survival-predictions/discussion/550003","metadata":{}},{"cell_type":"markdown","source":"# Pip Install Libraries for Metric\nSince internet must be turned off for submission, we pip install from my other notebook [here][1] where I downloaded the WHL files.\n\n[1]: https://www.kaggle.com/code/cdeotte/pip-install-lifelines","metadata":{}},{"cell_type":"code","source":"!pip install /kaggle/input/pip-install-lifelines/autograd-1.7.0-py3-none-any.whl\n!pip install /kaggle/input/pip-install-lifelines/autograd-gamma-0.5.0.tar.gz\n!pip install /kaggle/input/pip-install-lifelines/interface_meta-1.3.0-py3-none-any.whl\n!pip install /kaggle/input/pip-install-lifelines/formulaic-1.0.2-py3-none-any.whl\n!pip install /kaggle/input/pip-install-lifelines/lifelines-0.30.0-py3-none-any.whl","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-12-05T00:02:14.353744Z","iopub.execute_input":"2024-12-05T00:02:14.354389Z","iopub.status.idle":"2024-12-05T00:05:36.754907Z","shell.execute_reply.started":"2024-12-05T00:02:14.354356Z","shell.execute_reply":"2024-12-05T00:05:36.753948Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Train and Test","metadata":{}},{"cell_type":"code","source":"import numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)\n\ntest = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/test.csv\")\nprint(\"Test shape:\", test.shape )\n\ntrain = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/train.csv\")\nprint(\"Train shape:\",train.shape)\ntrain.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T00:05:36.756924Z","iopub.execute_input":"2024-12-05T00:05:36.757257Z","iopub.status.idle":"2024-12-05T00:05:37.035157Z","shell.execute_reply.started":"2024-12-05T00:05:36.757227Z","shell.execute_reply":"2024-12-05T00:05:37.034285Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EDA on Train Targets\nThere are two train targets `efs` and `efs_time`. When `efs==1` we know patient **did not survive** and we know time of death is `efs_time`. When `efs==0` we **do not know** if patient survived or not, but we do know that patient survived at least as long as `efs_time`.","metadata":{}},{"cell_type":"code","source":"plt.hist(train.loc[train.efs==1,\"efs_time\"],bins=100,label=\"efs=1, Did Not Survive\")\nplt.hist(train.loc[train.efs==0,\"efs_time\"],bins=100,label=\"efs=0, Maybe Survived\")\nplt.xlabel(\"Time of Observation, efs_time\")\nplt.ylabel(\"Density\")\nplt.title(\"Times of Observation. Either time to death, or time observed alive.\")\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T00:05:37.036273Z","iopub.execute_input":"2024-12-05T00:05:37.036559Z","iopub.status.idle":"2024-12-05T00:05:37.551722Z","shell.execute_reply.started":"2024-12-05T00:05:37.036532Z","shell.execute_reply":"2024-12-05T00:05:37.550917Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Transform Two Train Targets into One Target!\nBoth targets `efs` and `efs_time` provide useful information. We will tranform these two targets into a single target to train our model with.","metadata":{}},{"cell_type":"code","source":"train[\"y\"] = train.efs_time.values\nmx = train.loc[train.efs==1,\"efs_time\"].max()\nmn = train.loc[train.efs==0,\"efs_time\"].min()\ntrain.loc[train.efs==0,\"y\"] = train.loc[train.efs==0,\"y\"] + mx - mn\ntrain.y = train.y.rank()\ntrain.loc[train.efs==0,\"y\"] += len(train)//2\ntrain.y = train.y / train.y.max()\n\nplt.hist(train.loc[train.efs==1,\"y\"],bins=100,label=\"efs=1, Did Not Survive\")\nplt.hist(train.loc[train.efs==0,\"y\"],bins=100,label=\"efs=0, Maybe Survived\")\nplt.xlabel(\"Transformed Target y\")\nplt.ylabel(\"Density\")\nplt.title(\"Transformed Target y using both efs and efs_time.\")\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T00:05:37.552973Z","iopub.execute_input":"2024-12-05T00:05:37.553341Z","iopub.status.idle":"2024-12-05T00:05:38.16497Z","shell.execute_reply.started":"2024-12-05T00:05:37.553301Z","shell.execute_reply":"2024-12-05T00:05:38.164246Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Features\nThere are a total of 57 features. From these 35 are categorical and 22 are numerical. We will label encode the categorical features. Then our XGB and CAT model will accept these as categorical features and process them special internally. We leave the numerical feature NANs as NANs because GBDT (like XGB and CAT) can handle NAN and will use this information.","metadata":{}},{"cell_type":"code","source":"RMV = [\"ID\",\"efs\",\"efs_time\",\"y\"]\nFEATURES = [c for c in train.columns if not c in RMV]\nprint(f\"There are {len(FEATURES)} FEATURES: {FEATURES}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T00:05:38.167218Z","iopub.execute_input":"2024-12-05T00:05:38.167596Z","iopub.status.idle":"2024-12-05T00:05:38.172691Z","shell.execute_reply.started":"2024-12-05T00:05:38.167555Z","shell.execute_reply":"2024-12-05T00:05:38.171856Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CATS = []\nfor c in FEATURES:\n    if train[c].dtype==\"object\":\n        CATS.append(c)\n        train[c] = train[c].fillna(\"NAN\")\n        test[c] = test[c].fillna(\"NAN\")\nprint(f\"In these features, there are {len(CATS)} CATEGORICAL FEATURES: {CATS}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T00:05:38.173717Z","iopub.execute_input":"2024-12-05T00:05:38.17396Z","iopub.status.idle":"2024-12-05T00:05:38.263867Z","shell.execute_reply.started":"2024-12-05T00:05:38.173935Z","shell.execute_reply":"2024-12-05T00:05:38.263038Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"combined = pd.concat([train,test],axis=0,ignore_index=True)\n#print(\"Combined data shape:\", combined.shape )\n\n# LABEL ENCODE CATEGORICAL FEATURES\nprint(\"We LABEL ENCODE the CATEGORICAL FEATURES: \",end=\"\")\nfor c in FEATURES:\n\n    # LABEL ENCODE CATEGORICAL AND CONVERT TO INT32 CATEGORY\n    if c in CATS:\n        print(f\"{c}, \",end=\"\")\n        combined[c],_ = combined[c].factorize()\n        combined[c] -= combined[c].min()\n        combined[c] = combined[c].astype(\"int32\")\n        combined[c] = combined[c].astype(\"category\")\n        \n    # REDUCE PRECISION OF NUMERICAL TO 32BIT TO SAVE MEMORY\n    else:\n        if combined[c].dtype==\"float64\":\n            combined[c] = combined[c].astype(\"float32\")\n        if combined[c].dtype==\"int64\":\n            combined[c] = combined[c].astype(\"int32\")\n    \ntrain = combined.iloc[:len(train)].copy()\ntest = combined.iloc[len(train):].reset_index(drop=True).copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T00:07:49.647807Z","iopub.execute_input":"2024-12-05T00:07:49.648443Z","iopub.status.idle":"2024-12-05T00:07:49.736115Z","shell.execute_reply.started":"2024-12-05T00:07:49.648394Z","shell.execute_reply":"2024-12-05T00:07:49.735297Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# XGBoost\nWe train XGBoost model with CV 0.666","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom xgboost import XGBRegressor, XGBClassifier\nimport xgboost\nprint(\"Using XGBoost version\",xgboost.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T23:48:31.603639Z","iopub.execute_input":"2024-12-04T23:48:31.603866Z","iopub.status.idle":"2024-12-04T23:48:32.292613Z","shell.execute_reply.started":"2024-12-04T23:48:31.603843Z","shell.execute_reply":"2024-12-04T23:48:32.291755Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nFOLDS = 5\nkf = KFold(n_splits=FOLDS, shuffle=True, random_state=42)\n    \noof_xgb = np.zeros(len(train))\npred_xgb = np.zeros(len(test))\n\nfor i, (train_index, test_index) in enumerate(kf.split(train)):\n\n    print(\"#\"*25)\n    print(f\"### Fold {i+1}\")\n    print(\"#\"*25)\n    \n    x_train = train.loc[train_index,FEATURES].copy()\n    y_train = train.loc[train_index,\"y\"]\n    x_valid = train.loc[test_index,FEATURES].copy()\n    y_valid = train.loc[test_index,\"y\"]\n    x_test = test[FEATURES].copy()\n\n    model_xgb = XGBRegressor(\n        device=\"cuda\",\n        max_depth=3,  \n        colsample_bytree=0.5, \n        subsample=0.8, \n        n_estimators=10_000,  \n        learning_rate=0.1, \n        eval_metric=\"mae\",\n        early_stopping_rounds=25,\n        objective='reg:logistic',\n        enable_categorical=True,\n        min_child_weight=5\n    )\n    model_xgb.fit(\n        x_train, y_train,\n        eval_set=[(x_valid, y_valid)],  \n        verbose=100 \n    )\n\n    # INFER OOF\n    oof_xgb[test_index] = model_xgb.predict(x_valid)\n    # INFER TEST\n    pred_xgb += model_xgb.predict(x_test)\n\n# COMPUTE AVERAGE TEST PREDS\npred_xgb /= FOLDS","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T23:48:32.293642Z","iopub.execute_input":"2024-12-04T23:48:32.294027Z","iopub.status.idle":"2024-12-04T23:48:40.112671Z","shell.execute_reply.started":"2024-12-04T23:48:32.293998Z","shell.execute_reply":"2024-12-04T23:48:40.111812Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from metric import score\n\ny_true = train[[\"ID\",\"efs\",\"efs_time\",\"race_group\"]].copy()\ny_pred = train[[\"ID\"]].copy()\ny_pred[\"prediction\"] = -oof_xgb\nm = score(y_true.copy(), y_pred.copy(), \"ID\")\nprint(f\"\\nOverall CV for XGBoost =\",m)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T23:48:40.113772Z","iopub.execute_input":"2024-12-04T23:48:40.11405Z","iopub.status.idle":"2024-12-04T23:48:40.513989Z","shell.execute_reply.started":"2024-12-04T23:48:40.114022Z","shell.execute_reply":"2024-12-04T23:48:40.513035Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_importance = model_xgb.feature_importances_\nimportance_df = pd.DataFrame({\n    \"Feature\": FEATURES,  # Replace FEATURES with your list of feature names\n    \"Importance\": feature_importance\n}).sort_values(by=\"Importance\", ascending=False)\nplt.figure(figsize=(10, 15))\nplt.barh(importance_df[\"Feature\"], importance_df[\"Importance\"])\nplt.xlabel(\"Importance\")\nplt.ylabel(\"Feature\")\nplt.title(\"XGBoost Feature Importance\")\nplt.gca().invert_yaxis()  # Flip features for better readability\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T23:48:40.515036Z","iopub.execute_input":"2024-12-04T23:48:40.515458Z","iopub.status.idle":"2024-12-04T23:48:41.150371Z","shell.execute_reply.started":"2024-12-04T23:48:40.515428Z","shell.execute_reply":"2024-12-04T23:48:41.149439Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CatBoost\nWe train CatBoost model with CV 0.665","metadata":{}},{"cell_type":"code","source":"from catboost import CatBoostRegressor, CatBoostClassifier\nimport catboost\nprint(\"Using CatBoost version\",catboost.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T23:48:41.151645Z","iopub.execute_input":"2024-12-04T23:48:41.152014Z","iopub.status.idle":"2024-12-04T23:48:41.406379Z","shell.execute_reply.started":"2024-12-04T23:48:41.151974Z","shell.execute_reply":"2024-12-04T23:48:41.405495Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nFOLDS = 5\nkf = KFold(n_splits=FOLDS, shuffle=True, random_state=42)\n    \noof_cat = np.zeros(len(train))\npred_cat = np.zeros(len(test))\n\nfor i, (train_index, test_index) in enumerate(kf.split(train)):\n\n    print(\"#\"*25)\n    print(f\"### Fold {i+1}\")\n    print(\"#\"*25)\n    \n    x_train = train.loc[train_index,FEATURES].copy()\n    y_train = train.loc[train_index,\"y\"]\n    x_valid = train.loc[test_index,FEATURES].copy()\n    y_valid = train.loc[test_index,\"y\"]\n    x_test = test[FEATURES].copy()\n\n    model_cat = CatBoostRegressor(\n        task_type=\"GPU\",  \n    )\n    model_cat.fit(x_train,y_train,\n              eval_set=(x_valid, y_valid),\n              cat_features=CATS,\n              verbose=100)\n\n    # INFER OOF\n    oof_cat[test_index] = model_cat.predict(x_valid)\n    # INFER TEST\n    pred_cat += model_cat.predict(x_test)\n\n# COMPUTE AVERAGE TEST PREDS\npred_cat /= FOLDS","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T23:48:41.407411Z","iopub.execute_input":"2024-12-04T23:48:41.407679Z","iopub.status.idle":"2024-12-04T23:51:45.939763Z","shell.execute_reply.started":"2024-12-04T23:48:41.407655Z","shell.execute_reply":"2024-12-04T23:51:45.938777Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_true = train[[\"ID\",\"efs\",\"efs_time\",\"race_group\"]].copy()\ny_pred = train[[\"ID\"]].copy()\ny_pred[\"prediction\"] = -oof_cat\nm = score(y_true.copy(), y_pred.copy(), \"ID\")\nprint(f\"\\nOverall CV for CatBoost =\",m)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T23:51:45.941057Z","iopub.execute_input":"2024-12-04T23:51:45.9414Z","iopub.status.idle":"2024-12-04T23:51:46.54321Z","shell.execute_reply.started":"2024-12-04T23:51:45.941365Z","shell.execute_reply":"2024-12-04T23:51:46.541667Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CatBoost Feature Importance","metadata":{}},{"cell_type":"code","source":"feature_importance = model_cat.get_feature_importance()\nimportance_df = pd.DataFrame({\n    \"Feature\": FEATURES, \n    \"Importance\": feature_importance\n}).sort_values(by=\"Importance\", ascending=False)\nplt.figure(figsize=(10, 15))\nplt.barh(importance_df[\"Feature\"], importance_df[\"Importance\"])\nplt.xlabel(\"Importance\")\nplt.ylabel(\"Feature\")\nplt.title(\"CatBoost Feature Importance\")\nplt.gca().invert_yaxis()  # Flip features for better readability\nplt.show()","metadata":{"trusted":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-12-04T23:51:46.545078Z","iopub.execute_input":"2024-12-04T23:51:46.545402Z","iopub.status.idle":"2024-12-04T23:51:47.204765Z","shell.execute_reply.started":"2024-12-04T23:51:46.545368Z","shell.execute_reply":"2024-12-04T23:51:47.203975Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Ensemble CAT and XGB\nWe ensemble our XGBoost and CatBoost to achieve CV 0.668!","metadata":{}},{"cell_type":"code","source":"y_true = train[[\"ID\",\"efs\",\"efs_time\",\"race_group\"]].copy()\ny_pred = train[[\"ID\"]].copy()\ny_pred[\"prediction\"] = -oof_xgb -oof_cat\nm = score(y_true.copy(), y_pred.copy(), \"ID\")\nprint(f\"\\nOverall CV for Ensemble =\",m)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T23:51:47.206011Z","iopub.execute_input":"2024-12-04T23:51:47.206627Z","iopub.status.idle":"2024-12-04T23:51:47.519432Z","shell.execute_reply.started":"2024-12-04T23:51:47.206587Z","shell.execute_reply":"2024-12-04T23:51:47.518591Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create Submission CSV","metadata":{}},{"cell_type":"code","source":"sub = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/sample_submission.csv\")\nsub.prediction = -pred_xgb -pred_cat\nsub.to_csv(\"submission.csv\",index=False)\nprint(\"Sub shape:\",sub.shape)\nsub.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T23:51:47.520496Z","iopub.execute_input":"2024-12-04T23:51:47.520765Z","iopub.status.idle":"2024-12-04T23:51:47.540482Z","shell.execute_reply.started":"2024-12-04T23:51:47.52074Z","shell.execute_reply":"2024-12-04T23:51:47.539691Z"}},"outputs":[],"execution_count":null}]}